{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "We used this dataset for our miniproject: https://www.kaggle.com/datasets/puneet6060/intel-image-classification\n",
    "\n",
    "The dataset contains 6 classes of images: `building`, `forest`, `glacier`, `mountain`, `sea`, and `street`. We decided to delete the class `street` and it's corresponding pictures to simplify the learning process. The images are divided into folders with their respective labels. We used the `image_dataset_from_directory` method of Keras to convert the images into a TensorFlow dataset object for training.\n",
    "\n",
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_img_path = \"./dataset/seg_train/seg_train/\"\n",
    "\n",
    "img_size = 50\n",
    "batch_size = 32\n",
    "seed = 42   # the seed will make sure the two datasets are not overlapping\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    training_img_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    seed=seed,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    training_img_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    seed=seed,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(train_ds.class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first few images of the training dataset. Here, we'll define a function that we'll use again later in the notebook.\n",
    "\n",
    "To ensure the model won't eat up too many of our computer's resources, we downscaled the images from $150 \\times 150px$ to $50 \\times 50px$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset():\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  for images, labels in train_ds.take(1):\n",
    "    for i in range(12):\n",
    "      ax = plt.subplot(3, 4, i + 1)\n",
    "      plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "      plt.title(train_ds.class_names[labels[i]])\n",
    "      plt.axis(\"off\")\n",
    "\n",
    "show_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting Model\n",
    "We will create a model that is too simple and won't describe the data accurately enough."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "We start with a convolutional layer that'll import/rescale the images to `img_size`, using the ReLU activation function (for all the convolution and dense layers).\n",
    "\n",
    "Then, run a kernel of $3 \\times 3$ over each image 16 times. `padding=same` is referring to the padding of the image (needed because of the kernel) being filled with zeros.\n",
    "\n",
    "Following that is another convolution layer and max pooling layer. Next, the image will be flattened into a vector, ready to be fed to the following dense layer. In this case, we first wanted to apply dropout to the layer, but we ended up just halfing the amount of nodes of that layer. Why? We learned that the difference between applying `dropout(0.5)` and halving the nodes of the layer is that dropout randomly drops out nodes during each training iteration, which means that **different** nodes will be dropped out in each iteration. This allows the network to learn more robust and generalizable representations of the data, as it is forced to rely on a subset of nodes in each iteration, which prevents overfitting. While halving the nodes reduces the number of nodes in the layer permanently, which means that the network has less capacity to learn and represent complex patterns in the data. This then leads to underfitting, where the model is not able to capture the important features in the data. Which is exactly what we want. Also, dropout is present during training, but not during inference.\n",
    "\n",
    "The last dense layer represents the output layer, having a shared softmax activation layer to determine the probabilities of the 5 different classes.\n",
    "\n",
    "This task was interesting because we initially built a model that was too simple, even for underfitting. After the second epoch, it couldn't learn any more information because it simply was not complex enough. As a result, the accuracy stalled at around 0.25 and stayed the same for all the remaining epochs. We were not happy with that, so we now created a model that can actually improve with each epoch while still underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(6, (3,3), input_shape=(img_size,img_size,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(6, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "model._name='underfitting_model'\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and analyze the model\n",
    "Now, we'll have a look at how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_and_accuracy():\n",
    "    # Plot the training and validation Loss\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Train','Val'], loc= 'upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the model accuracy\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "visualize_loss_and_accuracy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, in the beginning, there's a gap in between the the validation and training loss and also in the accuracy. That is because of the randomness of the two different sample sets.\n",
    "\n",
    "We can see from the learning curves that the model slowly converges and is indeed underfitting. Both the training and validation accuracy curves plateau at a low value, indicating that the model is not learning the patterns in the data well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_img_path = \"./dataset/seg_test/seg_test/\"\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    testing_img_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode='int',\n",
    "    seed=seed,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "predictions = model.predict(test_ds)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "print('Predicted classes:', predicted_classes)\n",
    "print('True labels:', true_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    accuracy = model.evaluate(test_ds)[1]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "evaluate_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.as_numpy_iterator\n",
    "\n",
    "print('Classification Report:\\n', metrics.classification_report(y_true=true_classes, y_pred=predicted_classes))\n",
    "\n",
    "#https://www.kaggle.com/code/avantikab/intel-image-classification-cnn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is another way to visualize the performance of the model and wether it is underfitting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_confusion_matrix():\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true=true_classes, y_pred=predicted_classes)\n",
    "    display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=test_ds.class_names) #important, what mr lehmann said\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "\n",
    "print_confusion_matrix()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no clear diagonal line in the middle, in fact, the matrix shows that the mapping of the classes is all over the place. Which again confirms that this model is indeed underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4d5e30736655918a4e9f405c8a444b278be5101bf9903a41ed4e49b04d98f49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
